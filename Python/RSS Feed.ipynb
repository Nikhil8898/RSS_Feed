{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f6e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This program extract feed text and other information from RSS feeds.\n",
    "##It inserts the extracted information into database table\n",
    "##\n",
    "##created by: Dipen Vadodaria\n",
    "##created date: 19-mar-2013\n",
    "##updated for enclosures on 09-may-2013\n",
    "##updated to get content and image from the content on 14-may-2013\n",
    "\n",
    "import feedparser\n",
    "import re\n",
    "import mysql.connector\n",
    "#import sqlite3\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#import DBuser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(user='root',db='rss_feed',host='localhost',port=\"3306\", passwd='Admin@123',charset='utf8',use_unicode=True)\n",
    "#db = MySQLdb.connect(user='root',passwd='root',db='athena',charset='utf8',use_unicode=True)\n",
    "#db = sqlite3.connect('rssfeed.sqlite')\n",
    "#prepare cursors for read and write\n",
    "\n",
    "cursor_r = db.cursor()\n",
    "cursor_w = db.cursor()\n",
    "cursor_u = db.cursor()\n",
    "\n",
    "#prepare SQL query to select feed_url\n",
    "#print sys.argv[1]\n",
    "frequency = sys.argv[1]\n",
    "myread = [{\"frequency\" : frequency}]\n",
    "sql_read=\"select feed_catalogue_id, feed_url, feed_etag, feed_last_modified, updated_time, upper(scrap_page) from feed_catalogue where upper(is_active='YES') and frequency = %(frequency)s\" \n",
    "\n",
    "try:\n",
    "        #execute SQL commond\n",
    "        #cursor_r.executemany(sql_read,myread)\n",
    "        #fetch all rows\n",
    "        results = cursor_r.fetchall()\n",
    "        for row in results:\n",
    "                feed_id = row[0]\n",
    "                feed_url = row[1]\n",
    "                e_tag = row[2]\n",
    "                last_modified = row[3]\n",
    "                last_updated_time = row[4]\n",
    "                scrap_flag = row[5]\n",
    "                new_url = feed_url\n",
    "                #print result\n",
    "                print (\"id=%d,url=%s\" % (feed_id, feed_url))\n",
    "                last_updated_time =  last_updated_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                #feedparser.USER_AGENT = \"Feeds/1.0 +http://iookii.com/\"\n",
    "                \n",
    "                d=feedparser.parse(feed_url,etag=e_tag,modified=last_modified)\n",
    "                \n",
    "                ##following two lines were used for testing\n",
    "                #d=feedparser.parse(\"http://feeds.abcnews.com/abcnews/topstories\")\n",
    "                #feed_id=227 \n",
    "                \n",
    "                if 'status' in d.feed:\n",
    "                        if d.status == 304:\n",
    "                                # skip if feed has not changed\n",
    "                                continue\n",
    "\n",
    "                        if d.status == 301 :\n",
    "                                #url has permenently changed update database with the new one\n",
    "                                new_url = d.href\n",
    "                        else :\n",
    "                                new_url = feed_url\n",
    "                        \n",
    "                #get the last modified and e-tags to see if the feed has been modified when accesssed next time\n",
    "                        \n",
    "                last_modified = d.feed.get('modified','no modified')\n",
    "                e_tag = d.feed.get('etag','no etag')\n",
    "\n",
    "                date_last_modified = None #use this if individual feeds do not have dates\n",
    "                \n",
    "                if 'modified_parsed' in d.feed and d.feed.modified_parsed != None:\n",
    "                        print ('modified time')\n",
    "                        feed_year=d.feed.modified_parsed.tm_year\n",
    "                        feed_month=d.feed.modified_parsed.tm_mon\n",
    "                        feed_day=d.feed.modified_parsed.tm_mday\n",
    "                        feed_hour=d.feed.modified_parsed.tm_hour\n",
    "                        feed_minute=d.feed.modified_parsed.tm_min\n",
    "                        feed_second=d.feed.modified_parsed.tm_sec\n",
    "\n",
    "                        date_last_modified = str(feed_year)+\"-\"+str(feed_month).zfill(2)+\"-\"+ str(feed_day).zfill(2)+\" \"+ str(feed_hour).zfill(2)+\":\"+str(feed_minute).zfill(2)+\":\"+str(feed_second).zfill(2)\n",
    "\n",
    "                updated_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                sql_update=\"update salt_feed_catalogue \\\n",
    "                                set feed_url = '%s', \\\n",
    "                                feed_etag= '%s', \\\n",
    "                                feed_last_modified = '%s',\\\n",
    "                                updated_time = '%s' \\\n",
    "                                where feed_catalogue_id = '%d'\" %\\\n",
    "                                (new_url,e_tag,last_modified,updated_time, feed_id)\n",
    "                try:\n",
    "                        cursor_u.execute(sql_update)\n",
    "                        db.commit()\n",
    "                except Exception:\n",
    "                        print (sql_update)\n",
    "                        traceback.print_exc()\n",
    "                        db.rollback()\n",
    "                        break\n",
    "                \n",
    "                for e in d.entries :\n",
    "                        try:\n",
    "                                                       \n",
    "                                image_url = None\n",
    "                                \n",
    "                                s=e.summary\n",
    "\n",
    "                                soup=bs(s)\n",
    "\n",
    "                                try:\n",
    "                                        if 'published_parsed' in e:\n",
    "                                                print ('in pub date')\n",
    "                                                feed_date_published=e.published\n",
    "                                                feed_year=e.published_parsed.tm_year\n",
    "                                                feed_month=e.published_parsed.tm_mon\n",
    "                                                feed_day=e.published_parsed.tm_mday\n",
    "                                                feed_hour=e.published_parsed.tm_hour\n",
    "                                                feed_minute=e.published_parsed.tm_min\n",
    "                                                feed_second=e.published_parsed.tm_sec\n",
    "                                        else:\n",
    "                                                print ('in date')\n",
    "                                                feed_date_published=e.date\n",
    "                                                feed_year=e.date_parsed.tm_year\n",
    "                                                feed_month=e.date_parsed.tm_mon\n",
    "                                                feed_day=e.date_parsed.tm_mday\n",
    "                                                feed_hour=e.date_parsed.tm_hour\n",
    "                                                feed_minute=e.date_parsed.tm_min\n",
    "                                                feed_second=e.date_parsed.tm_sec\n",
    "                                        date_published = str(feed_year)+\"-\"+str(feed_month).zfill(2)+\"-\"+ str(feed_day).zfill(2)+\" \"+ str(feed_hour).zfill(2)+\":\"+str(feed_minute).zfill(2)+\":\"+str(feed_second).zfill(2)\n",
    "                                        print (date_published)\n",
    "                                        print (last_updated_time)\n",
    "                                except Exception:\n",
    "                                        print ('in date exception')\n",
    "                                        if date_last_modified != None:\n",
    "                                                #if last modified is available in feeds at the top use it\n",
    "                                                print (\"modified date in the header\")\n",
    "                                                date_published = date_last_modified\n",
    "                                        else:\n",
    "                                                if (feed_id >= 1436 and feed_id <= 1449):\n",
    "                                                        print ('sandesh')\n",
    "                                                        print (feed_date_published)\n",
    "                                                        dt =  datetime.strptime(feed_date_published.replace(' ', '')[:-3],'%m/%d/%Y%I:%M:%S%p') #remove IST with replace\n",
    "                                                        t=timedelta(minutes=330)\n",
    "                                                        date_published = dt-t\n",
    "                                                        date_published = date_published.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                                        print (date_published)\n",
    "                                                else:\n",
    "                                                        if (feed_id >= 1123 and feed_id <= 1141):\n",
    "                                                                print ('sakal')\n",
    "                                                                print (feed_date_published)\n",
    "                                                                dt =  datetime.strptime(feed_date_published[:-6],'%a,%d %b %Y %H:%M:%S') #remove +0550 with [-5]\n",
    "                                                                t=timedelta(minutes=330)\n",
    "                                                                date_published = dt-t\n",
    "                                                                date_published = date_published.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                                                print (date_published)\n",
    "                                                        else:\n",
    "                                                                date_published = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "                                \n",
    "                                if (date_published > last_updated_time) and (date_published <= datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')):\n",
    "                                        \n",
    "                                        try:\n",
    "                                                #get image embeded in description..ignore if width is given and is less than 90\n",
    "                                                for image in soup.findAll(\"img\"):\n",
    "                                                        if image.has_key(\"width\") :\n",
    "                                                                if int(image[\"width\"].replace(\"px\",\"\")) > 75:  #accept this image only if width is large enough\n",
    "                                                                        image_url = image[\"src\"]\n",
    "                                                                        break;\n",
    "                                                        else:\n",
    "                                                                filename = image[\"src\"].split(\"/\")[-1]\n",
    "                                                                file_ext=filename.split(\".\")[-1]\n",
    "                                                                if (file_ext == \"jpg\" or file_ext == \"jpeg\") or (feed_id > 1176 and feed_id < 1239 and file_ext == \"cms\") or (feed_id > 1397 and feed_id < 1407 and file_ext == \"cms\"):\n",
    "                                                                        image_url = image[\"src\"]\n",
    "                                                                        #print \"got the image 1\"\n",
    "                                                                        break;\n",
    "                                        except Exception:\n",
    "                                                traceback.print_exc()\n",
    "                                        \n",
    "                                        feed_text=re.compile(r'<[^>]+>').sub('',s)\n",
    "                                        #feed_text=re.compile(r\"'\").sub(\"\\\\'\",feed_text)\n",
    "                                        feed_text=re.compile(r\"&nbsp;\").sub(' ',feed_text) #ignore &nbsp\n",
    "                                        feed_text=bs(feed_text,convertEntities=bs.HTML_ENTITIES)\n",
    "                                        #feed_text=feed_text.encode('utf-8','ignore')\n",
    "\n",
    "                                        feed_title=re.compile(r'<[^>]+>').sub('',e.title)\n",
    "                                        #feed_title=re.compile(r\"'\").sub(\"\\\\'\",feed_title)\n",
    "                                        feed_title=re.compile(r\"&nbsp;\").sub(' ',feed_title) #ignore &nbsp\n",
    "                                        feed_title=bs(feed_title,convertEntities=bs.HTML_ENTITIES)\n",
    "                                        #feed_title=feed_title.encode('utf-8','ignore')\n",
    "                                        \n",
    "                                        #feed_text_url=re.compile(r\"'\").sub(\"\\\\'\",e.link)\n",
    "                                        feed_text_url=e.link\n",
    "                                        \n",
    "                                        #if e.published.find(\"GMT\") > -1:\n",
    "        ##                                try:\n",
    "        ##                                        feed_date_published=e.published\n",
    "        ##                                        feed_year=e.published_parsed.tm_year\n",
    "        ##                                        feed_month=e.published_parsed.tm_mon\n",
    "        ##                                        feed_day=e.published_parsed.tm_mday\n",
    "        ##                                        feed_hour=e.published_parsed.tm_hour\n",
    "        ##                                        feed_minute=e.published_parsed.tm_min\n",
    "        ##                                        feed_second=e.published_parsed.tm_sec\n",
    "        ##                                        date_published = str(feed_year)+\"-\"+str(feed_month)+\"-\"+str(feed_day)+\" \"+str(feed_hour)+\":\"+str(feed_minute)+\":\"+str(feed_second)\n",
    "        ##                                except Exception:\n",
    "        ##                                        date_published = created_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                                        feed_content=\" \"\n",
    "                                        #image_url = None\n",
    "\n",
    "\n",
    "                                        #if media content tag is specified get url as it most likely is an image\n",
    "                                        if 'media_content' in e:\n",
    "                                                for content in e.media_content:\n",
    "                                                        if 'url' in content:\n",
    "                                                                file_ext = content['url'].split(\".\")[-1]\n",
    "                                                                if (file_ext == \"jpg\" or file_ext == \"jpeg\" or file_ext == \"gif\"):\n",
    "                                                                        image_url = content['url']\n",
    "                                                                        #print \"got the image 2\"\n",
    "                                                                        break;\n",
    "                                                      \n",
    "                                        try:\n",
    "                                                #if content is present get the content and image embedded in it\n",
    "                                                if 'content' in e:\n",
    "                                                        for c in e.content :\n",
    "                                                                #print \"in content\"\n",
    "                                                                try:\n",
    "                                                                        #content=re.compile(r'<[^>]+>').sub('',c.value)\n",
    "                                                                        #follwing one line commented on 08 sept 2013\n",
    "                                                                        #content=re.compile(r\"'\").sub(\"\\\\'\",c.value)\n",
    "                                                                        #content = unicode(content,\"utf-8\")\n",
    "                                                                        content=content.encode(\"utf-8\",\"ignore\")\n",
    "                                                                        #content=c.value\n",
    "                                                                        feed_content=feed_content+content\n",
    "                                                                        #print c.value\n",
    "                                                                except Exception:\n",
    "                                                                        traceback.print_exc()\n",
    "                                                        \n",
    "                                                                #image_url = None\n",
    "                                                                soup=bs(feed_content)\n",
    "                                        \n",
    "                                                                for image in soup.findAll(\"img\"):\n",
    "                                                                        if image.has_key(\"width\") :\n",
    "                                                                                if int(image[\"width\"].replace(\"px\",\"\")) > 75:  #accept this image only if width is large enough\n",
    "                                                                                        image_url = image[\"src\"]\n",
    "                                                                                        print (\"got the image 3\")\n",
    "                                                                                        #print image_url\n",
    "                                                                                        break;\n",
    "                                                                        else:\n",
    "                                                                                image_url = image[\"src\"]\n",
    "                                                                                print (\"got the image 3a\")\n",
    "                                                                                break\n",
    "                                        except Exception:\n",
    "                                                traceback.print_exc()\n",
    "                                                \n",
    "                                        if image_url == None:\n",
    "                                                #check the enclosure tag for image\n",
    "                                                if 'enclosures' in e:\n",
    "                                                        for encl in e.enclosures :\n",
    "                                                                if encl.type == \"image/jpeg\" or encl.type == \"image/jpg\":\n",
    "                                                                        image_url = encl.href\n",
    "                                                                        print (\"image in enclosure\")\n",
    "                                                                        break\n",
    "\n",
    "                                        if scrap_flag == \"YES\":\n",
    "                                                if image_url == None:\n",
    "                                                        soup = bs(urlopen(feed_text_url))\n",
    "                                                        parsed = list(urlparse.urlparse(feed_text_url))\n",
    "                                                        image_url=None\n",
    "\n",
    "                                                        for ptag in soup.findAll(\"p\"):\n",
    "                                                        \n",
    "                                                                try:\n",
    "                                                                        #go thru all ptags to find an image and stop once found\n",
    "                                                                        for image in ptag.findAll(\"img\"):\n",
    "                                                                                if image.has_key(\"width\") :\n",
    "                                                                                        #print image[\"width\"]\n",
    "                                                                                        if feed_id > 1035 and feed_id < 1057:\n",
    "                                                                                                #for the hindu \n",
    "                                                                                                w_size=325\n",
    "                                                                                        else:\n",
    "                                                                                                w_size=150\n",
    "                                                                                        if int(image[\"width\"].replace(\"px\",\"\")) > w_size:\n",
    "                                                                                                #print image\n",
    "                                                                                                filename = image[\"src\"].split(\"/\")[-1]\n",
    "                                                                                                file_ext=filename.split(\".\")[-1]\n",
    "                                                                                                if (file_ext == \"jpg\" or file_ext == \"jpeg\" or file_ext == \"gif\") and filename.find(\"-rss\") < 0 and filename.find(\"logo\") < 0 and (feed_id < 1036 or feed_id > 1056 or image[\"src\"].find(\"archive\") < 0):\n",
    "                                                                                                        parsed[2] = image[\"src\"]\n",
    "                                                                                                        #outpath = os.path.join(out_folder, filename)\n",
    "                                                                                                        if image[\"src\"].lower().startswith(\"http\"):\n",
    "                                                                                                                image_url=image[\"src\"]\n",
    "                                                                                                                #print \"got the image 4\"\n",
    "                                                                                                        else:\n",
    "                                                                                                                image_url=urlparse.urlunparse(parsed)\n",
    "                                                                                                                #print image_url\n",
    "                                                                                                                #print \"got the image 4a\"\n",
    "                                                                                                        break\n",
    "                                                                                else:\n",
    "                                                                                        #print image\n",
    "                                                                                        filename = image[\"src\"].split(\"/\")[-1]\n",
    "                                                                                        file_ext=filename.split(\".\")[-1]\n",
    "                                                                                        if (file_ext == \"jpg\" or file_ext == \"jpeg\" or file_ext == \"gif\") and filename.find(\"subscribe-facebook\") < 0 and filename.find(\"subscribe-twitter\") < 0 and filename.find(\"-rss\") < 0 and filename.find(\"logo\") < 0 and filename.find(\"-icon\") < 0 and (feed_id < 1036 or feed_id > 1056 or image[\"src\"].find(\"archive\") < 0):\n",
    "                                                                                                parsed[2] = image[\"src\"]\n",
    "                                                                                                #outpath = os.path.join(out_folder, filename)\n",
    "                                                                                                if image[\"src\"].lower().startswith(\"http\"):\n",
    "                                                                                                        image_url=image[\"src\"]\n",
    "                                                                                                        print (\"got the image 4b\")\n",
    "                                                                                                else:\n",
    "                                                                                                        image_url=urlparse.urlunparse(parsed)\n",
    "                                                                                                        print (image_url)\n",
    "                                                                                                        print (\"got the image 4c\")\n",
    "                                                                                                break\n",
    "                                                                except Exception:\n",
    "                                                                        traceback.print_exc()\n",
    "                                                                        break\n",
    "                                                        \n",
    "                                                                if image_url != None:\n",
    "                                                                        break\n",
    "                                                        \n",
    "                                                        if image_url == None:\n",
    "                                                                #print \"no url making last effort\"\n",
    "                                                                #if nothing works make the last effort scrap the page take first six and see if they are gif or jpg\n",
    "                                                        \n",
    "                                                                bodytag = soup.find(\"body\");\n",
    "\n",
    "                                                                for image in bodytag.findAll(\"img\",limit=12):\n",
    "                                                                        if image.has_key(\"width\") :\n",
    "                                                                                if feed_id > 1035 and feed_id < 1057:\n",
    "                                                                                        #for the hindu\n",
    "                                                                                        w_size=325\n",
    "                                                                                else:\n",
    "                                                                                        w_size=150\n",
    "                                                                                if int(image[\"width\"].replace(\"px\",\"\")) > w_size:\n",
    "                                                                                        #print \"Image: %(src)s\" % image\n",
    "                                                                                        filename = image[\"src\"].split(\"/\")[-1]\n",
    "                                                                                        file_ext=filename.split(\".\")[-1]\n",
    "                                                                                        if (file_ext == \"jpg\" or file_ext == \"jpeg\") and filename.find(\"logo\") < 0 and filename.find(\"-icon\") < 0 and (feed_id < 1036 or feed_id > 1056 or image[\"src\"].find(\"archive\") < 0):\n",
    "                                                                                                parsed[2] = image[\"src\"]\n",
    "                                                                                                #outpath = os.path.join(out_folder, filename)\n",
    "                                                                                                if image[\"src\"].lower().startswith(\"http\"):\n",
    "                                                                                                        image_url=image[\"src\"]\n",
    "                                                                                                else:\n",
    "                                                                                                        image_url=urlparse.urlunparse(parsed)\n",
    "                                                                                                #print image_url\n",
    "                                                                                                #print \"got the image 5\"\n",
    "                                                                                                break\n",
    "                                                                        else:\n",
    "                                                                                #print \"Image: %(src)s\" % image\n",
    "                                                                                filename = image[\"src\"].split(\"/\")[-1]\n",
    "                                                                                file_ext=filename.split(\".\")[-1]\n",
    "                                                                                if (file_ext == \"jpg\" or file_ext == \"jpeg\") and filename.find(\"subscribe-facebook\") < 0 and filename.find(\"subscribe-twitter\") < 0 and filename.find(\"logo\") < 0 and filename.find(\"-rss\") < 0 and filename.find(\"-icon\") < 0 and (feed_id < 1036 or feed_id > 1056 or image[\"src\"].find(\"archive\") < 0):\n",
    "                                                                                        parsed[2] = image[\"src\"]\n",
    "                                                                                        #outpath = os.path.join(out_folder, filename)\n",
    "                                                                                        if image[\"src\"].lower().startswith(\"http\"):\n",
    "                                                                                                image_url=image[\"src\"]\n",
    "                                                                                        else:\n",
    "                                                                                                image_url=urlparse.urlunparse(parsed)\n",
    "                                                                                        #print \"got the image 5b\"\n",
    "                                                                                        #print image_url\n",
    "                                                                                        break\n",
    "                        \n",
    "                                        #print feed_text\n",
    "\n",
    "                                        #if len(e.enclosures) > 0:\n",
    "                                        #        print e.enclosures[0]\n",
    "\n",
    "                                        #prepare SQL query to insert feed_text\n",
    "                                        try:\n",
    "                                                created_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                                myinsert = [{\"id\" : feed_id, \"url\": feed_text_url, \"text\": feed_text, \"title\": feed_title, \"content\": feed_content, \"image\": image_url, \"published\": date_published, \"created\": created_time }]\n",
    "                                                \n",
    "                                                sql_write=\"insert into eeds(feed_catalogue_id, feed_text_url, \\\n",
    "                                                feed_text, feed_title, feed_content, feed_image_file_path,feed_date_published,created_time) \\\n",
    "                                                values (%(id)s, %(url)s, %(text)s,%(title)s,%(content)s, %(image)s,%(published)s,%(created)s)\"\n",
    "                                                \n",
    "                                                \n",
    "                                        except Exception:\n",
    "                                                traceback.print_exc()\n",
    "                                        try:\n",
    "                                                cursor_w.execute(\"set names utf8;\") \n",
    "                                                cursor_w.executemany(sql_write,myinsert)\n",
    "                                                db.commit()\n",
    "                                        except Exception:\n",
    "                                                print (sql_write)\n",
    "                                                db.rollback()\n",
    "                                                traceback.print_exc()\n",
    "                        except Exception:\n",
    "                                traceback.print_exc()\n",
    "                                pass\n",
    "                #break\n",
    "except Exception:\n",
    "        #ignore exception and go forward\n",
    "        #pass\n",
    "        print (\"Error: Unable to extract feed data\")\n",
    "        traceback.print_exc()\n",
    "#disconnect\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9208d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.0.1-cp39-cp39-win_amd64.whl (354 kB)\n",
      "Installing collected packages: pymongo\n",
      "Successfully installed pymongo-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c7021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
